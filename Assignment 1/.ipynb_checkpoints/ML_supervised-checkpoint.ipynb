{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c63f24",
   "metadata": {},
   "source": [
    "## some methods that I will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de52682d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f9d25c3ac35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def plot_learning_curve_newdata(estimator, title, X, y, axes=None, ylim=None, cv=None,  n_jobs=None, scoring=None,\n\u001b[0;32m----> 2\u001b[0;31m     train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Census pay\", estimator2='', sp1='', sp2='', X2='', y2=''):\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "try:\n",
    "    from imblearn.under_sampling import RandomUnderSampler \n",
    "except:\n",
    "    !pip install imblearn\n",
    "\n",
    "def plot_learning_curve_newdata(estimator, title, X, y, axes=None, ylim=None, cv=None,  n_jobs=None, scoring=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Census pay\", estimator2='', sp1='', sp2='', X2='', y2=''):\n",
    "\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    tit = title +\" \" + d_name\n",
    "    axes.set_title(tit)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator, X, y,scoring=scoring,  cv=cv, n_jobs=n_jobs,   train_sizes=train_sizes,\n",
    "        return_times=True,)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    \n",
    "    train_sizes, train_scores2, test_scores2, fit_times2, _ = learning_curve(\n",
    "        estimator2, X2, y2,scoring=scoring,  cv=cv, n_jobs=n_jobs,   train_sizes=train_sizes,\n",
    "        return_times=True,)\n",
    "    train_scores_mean2 = np.mean(train_scores2, axis=1)\n",
    "    train_scores_std2 = np.std(train_scores2, axis=1)\n",
    "    test_scores_mean2 = np.mean(test_scores2, axis=1)\n",
    "    test_scores_std2 = np.std(test_scores2, axis=1)\n",
    "    fit_times_mean2 = np.mean(fit_times2, axis=1)\n",
    "    fit_times_std2 = np.std(fit_times2, axis=1)\n",
    "\n",
    "        # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std, alpha=0.1,  color=\"r\", )\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std, alpha=0.1, color=\"g\", )\n",
    "    \n",
    "    xlab = 'Training score ' + sp1\n",
    "    axes.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=xlab)\n",
    "    xlab2 = 'Cross validation score ' + sp1\n",
    "    axes.plot(train_sizes, test_scores_mean, \"o-\", color=\"g\", label=xlab2)\n",
    "    \n",
    "    \n",
    "    axes.fill_between(train_sizes2, train_scores_mean2 - train_scores_std2,\n",
    "        train_scores_mean2 + train_scores_std2, alpha=0.1,  color=\"r\", )\n",
    "    axes.fill_between(train_sizes2, test_scores_mean2 - test_scores_std2,\n",
    "        test_scores_mean2 + test_scores_std2, alpha=0.1, color=\"g\", )\n",
    "    \n",
    "    xlab = 'Training score ' + sp2\n",
    "    axes.plot(train_sizes2, train_scores_mean2, \"o--\", color=\"r\", label=xlab)\n",
    "    xlab2 = 'Cross validation score ' + sp2\n",
    "    axes.plot(train_sizes2, test_scores_mean2, \"o--\", color=\"g\", label=xlab2)\n",
    "  \n",
    "    axes.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee917eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,  n_jobs=None, scoring=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Census pay\", title_extra= ''):\n",
    "\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    tit = title +\" \" + d_name\n",
    "    axes.set_title(tit)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "        # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    xlab = 'Training score' + title_extra\n",
    "    axes.plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=xlab\n",
    "    )\n",
    "    xlab2 = 'Cross validation score' + title_extra\n",
    "    axes.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=xlab2\n",
    "    )\n",
    "    axes.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    return axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_curve(estimator, X, y, param_name, param_range, scoring, ds_name,  is_bar=False):\n",
    "    train_scores, test_scores = validation_curve(estimator, X, y,  param_name=param_name,\n",
    "                                                 param_range=param_range, scoring=scoring,)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    tit = \"Validation Curve for \" + ds_name\n",
    "    plt.title(tit)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "#     plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "\n",
    "    if is_bar == False:\n",
    "        plt.plot(param_range, train_scores_mean, label=\"Mean train scores\", color=\"darkorange\")\n",
    "        plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,      alpha=0.2,\n",
    "            color=\"darkorange\",  lw=lw )\n",
    "\n",
    "\n",
    "        plt.plot(param_range, test_scores_mean, label=\"Mean test scores\", color=\"navy\",)\n",
    "\n",
    "        plt.fill_between( param_range,  test_scores_mean - test_scores_std,  test_scores_mean + test_scores_std,\n",
    "            alpha=0.2, color=\"navy\",    lw=lw,  )\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        #------------  plotting ---------     \n",
    "        ax = plt.gca()\n",
    "        X = np.arange(0, len(param_range))\n",
    "\n",
    "        ax.bar(X, train_scores_mean, color = 'b', width = 0.25, label = 'Training Accuracy')\n",
    "        ax.bar(X+0.25, test_scores_mean, color = 'g', width = 0.25, label = \"Validation Accuracy\")\n",
    "        plt.xticks(X, param_range)\n",
    "    \n",
    "        plt.legend(loc='lower left')\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val_curve2(estimator, X, y, param_name, param_range, scoring, ds_name, depths):\n",
    "    train_scores, test_scores = validation_curve(estimator, X, y,  param_name=param_name,\n",
    "                                                 param_range=param_range, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    tit = \"Validation Curve for \" + ds_name\n",
    "    plt.title(tit)\n",
    "    xlab = param_name + ' depth'\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "#     plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "    param_range = depths\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Mean train scores\", color=\"darkorange\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,      alpha=0.2,\n",
    "        color=\"darkorange\",  lw=lw )\n",
    "\n",
    "\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Mean test scores\", color=\"navy\",)\n",
    "    \n",
    "    plt.fill_between( param_range,  test_scores_mean - test_scores_std,  test_scores_mean + test_scores_std,\n",
    "        alpha=0.2, color=\"navy\",    lw=lw,  )\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ffd43",
   "metadata": {},
   "source": [
    "## Reading Datasets\n",
    "\n",
    "Dataset 1:Adult-all csv: https://raw.githubusercontent.com/jbrownlee/Datasets/master/adult-all.csv\n",
    "\n",
    "Dataset 2: https://www.kaggle.com/code/aman909/body-performance \\\n",
    "Outputs are 4 with all features numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5208b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Read the data gill-attachmen\n",
    "\n",
    "Names1 = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "          'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'Result']\n",
    "data1 = pd.read_csv('adult-all.csv', header=None,  index_col=False, names = Names1)\n",
    "data1.drop_duplicates(inplace=True)\n",
    "y=data1['Result']\n",
    "X= data1[data1.columns[data1.columns != 'Result']]\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "data1 = pd.concat([X_res, y_res], axis=1)\n",
    "data1.head()\n",
    "\n",
    "data2 = pd.read_csv('bodyPerformance.csv')\n",
    "data2.drop_duplicates(inplace=True)\n",
    "data2.head()\n",
    "\n",
    "\n",
    "\n",
    "# sns.scatterplot(x=\"weight_kg\", y=\"height_cm\", hue= 'Clusters',  data=df, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dbfb5",
   "metadata": {},
   "source": [
    "### Cleaning datasets and replacing missing values\n",
    "Assuming that missing values are represented with '?' which is the case with dataset 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ replaces the missing values with the most frequent value ---------------\n",
    "def missing_vals(data1):\n",
    "    for col in data1.columns:\n",
    "        if '?' in set(data1[col]):\n",
    "            print (col)\n",
    "            print('? values in',col, \":\", data1[col].value_counts()['?'])\n",
    "            data1[col].replace('?', np.nan, inplace=True)\n",
    "            print('Replacing ? with most frequent:', data1[col].mode().iloc[0])\n",
    "            data1[col].fillna(data1[col].mode().iloc[0], inplace=True)\n",
    "            print()\n",
    "        elif data1[col].isna().any():\n",
    "            print (col, \": nan values present\")\n",
    "            \n",
    "            \n",
    "#---------- takes data and convert columns to categories and stores the mappings in a dict ---- \n",
    "def convert_cats_labels(data, columns=None, cats_names_dict={}):\n",
    "    d1c = pd.DataFrame()\n",
    "    if columns == None:\n",
    "        for col in data.columns:\n",
    "            d1c[col] = data[col].astype('category')\n",
    "            cats_names_dict[col] = dict(enumerate(d1c[col].cat.categories))\n",
    "            d1c[col] = d1c[col].cat.codes\n",
    "    \n",
    "    else:\n",
    "        for col in data.columns:\n",
    "            if col in columns:\n",
    "                d1c[col] = data[col].astype('category')\n",
    "                cats_names_dict[col] = dict(enumerate(d1c[col].cat.categories))\n",
    "                d1c[col] = d1c[col].cat.codes\n",
    "            else:\n",
    "                d1c[col] = data[col]\n",
    "                \n",
    "    return d1c, cats_names_dict\n",
    "            \n",
    "missing_vals(data1)\n",
    "missing_vals(data2)\n",
    "\n",
    "\n",
    "\n",
    "cols_to_cats_d1 = ['workclass', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "          'relationship', 'race', 'sex', 'native-country', 'Result']\n",
    "\n",
    "data1, codes_d1 = convert_cats_labels(data1, columns = cols_to_cats_d1)\n",
    "\n",
    "\n",
    "\n",
    "cols_to_cats_d2 = ['gender', 'class']\n",
    "\n",
    "data2, codes_d2 = convert_cats_labels(data2, columns = cols_to_cats_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003a962",
   "metadata": {},
   "source": [
    "### Splitting the data 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X_data1 = data1[data1.columns[data1.columns != 'Result']]\n",
    "y_data1 = data1.loc[:, 'Result']\n",
    "\n",
    "X_data2 = data2[data2.columns[data2.columns != 'class']]\n",
    "y_data2 = data2.loc[:, 'class']\n",
    "\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_data1, y_data1, test_size=0.3, random_state=38)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_data2, y_data2, test_size=0.3, random_state=38)\n",
    "print(\"shape for data1 => training:\", X_train1.shape, \" and testing:\", X_test1.shape)\n",
    "print(\"shape for data2 => training:\", X_train2.shape, \" and testing:\", X_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad11a0a",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed91f50",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb308ea",
   "metadata": {},
   "source": [
    "#### Unconstrained trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06630dde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traintimesd1 ={}\n",
    "testtimesd1 ={}\n",
    "traintimesd2={}\n",
    "testtimesd2 = {}\n",
    "\n",
    "final_accsd1 = {}\n",
    "final_accsd2 = {}\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree  import export_graphviz\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from graphviz import Source\n",
    "except:\n",
    "    ! pip3 install graphviz \n",
    "    \n",
    "from IPython.display import SVG\n",
    "\n",
    "def scoring_wo_cons_dt(X_train1, y_train1, X_test1, y_test1, data_name='Census pay'):\n",
    "    print(\"Without constraints for Dataset:\" + data_name)\n",
    "    clf = DecisionTreeClassifier(random_state=10)\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(X_train1, y_train1)\n",
    "    end = time.time()\n",
    "    trainingtime = end - start\n",
    "    print(\"Training time\", round(trainingtime, 4), \"seconds\")\n",
    "    \n",
    "    training_score = clf.score(X_train1, y_train1)\n",
    "    start = time.time()\n",
    "    testing_score = clf.score(X_test1, y_test1)\n",
    "    end = time.time()\n",
    "    testingtime = end - start\n",
    "    print(\"Scoring time\", round(testingtime, 4), \"seconds\")\n",
    "    \n",
    "    print(\"Training Accuracy\", training_score)\n",
    "    print(\"Testing Accuracy\", testing_score)\n",
    "    return clf\n",
    "\n",
    "          \n",
    "\n",
    "#--------------------- showing the image ----------------\n",
    "def show_tree(X_train1, clf, show=False, name=\"DTcensus_unconstrained\"):\n",
    "    if show:\n",
    "\n",
    "        plt.figure(figsize=(40,20))  \n",
    "        _ = tree.plot_tree(clf, feature_names = X_train1.columns, \n",
    "                     filled=True, fontsize=6, rounded = True)\n",
    "        plt.show()\n",
    "        plt.savefig(name+'.png')\n",
    "\n",
    "\n",
    "sklearn.set_config(print_changed_only=False)\n",
    "\n",
    "def print_tree_atts(clf):\n",
    "    print(\"Max depth\", clf.tree_.max_depth)\n",
    "    print(\"Number of nodes\", clf.tree_.node_count)\n",
    "    print(clf)\n",
    "\n",
    "          \n",
    "clf1_uc = scoring_wo_cons_dt(X_train1, y_train1, X_test1, y_test1, \"census\")\n",
    "print_tree_atts(clf1_uc)\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "clf2_uc = scoring_wo_cons_dt(X_train2, y_train2, X_test2, y_test2, \"BodyPerf\")\n",
    "print_tree_atts(clf2_uc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10782cef",
   "metadata": {},
   "source": [
    "#### Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth_range = list(range(3,15))\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state = 50)\n",
    "val_curve(tree, X_train1, y_train1, \"max_depth\", max_depth_range, \"accuracy\", \"Census pay data\")\n",
    "val_curve(tree, X_train2, y_train2, \"max_depth\", max_depth_range, \"accuracy\", \"Body performance data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77525f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "minsamplesplit_range = list(range(2,100,5))\n",
    "tree = DecisionTreeClassifier(random_state = 50, max_depth=10)\n",
    "val_curve(tree, X_train1, y_train1, \"min_samples_split\", minsamplesplit_range, \"accuracy\", \"Census pay\")\n",
    "val_curve(tree, X_train1, y_train1, \"min_samples_split\", minsamplesplit_range, \"accuracy\", \"Census pay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e30942",
   "metadata": {},
   "source": [
    "### Pruning the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2488e6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#--------------------------- early pruning ---------------- \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def prepruning_search (X_train1, y_train1, X_test1, y_test1, ds_name = 'census pay',\n",
    "                       need_training = False, name_file='decision_tree'):\n",
    "    if need_training == True:\n",
    "        param_dist = {\"max_depth\": [6, 10],\n",
    "                      \"min_samples_split\": [2,10, 50],\n",
    "                      \n",
    "                      \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "        # Instantiate a Decision Tree classifier: tree\n",
    "        tree = DecisionTreeClassifier(random_state = 50, min_samples_leaf= 20)\n",
    "\n",
    "        # Instantiate the RandomizedSearchCV object: tree_cv\n",
    "        tree_cv = RandomizedSearchCV(tree, param_dist,  n_iter=30, cv=5)\n",
    "        tree_cv.fit(X_train1, y_train1)\n",
    "        clf = tree_cv.best_estimator_\n",
    "    \n",
    "        start = time.time()\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        end = time.time()\n",
    "        trainingtime = end - start\n",
    "        print(\"Training time\", round(trainingtime, 4), \"seconds\")\n",
    "    \n",
    "        training_score = clf.score(X_train1, y_train1)\n",
    "        \n",
    "        start = time.time()\n",
    "        testing_score = clf.score(X_test1, y_test1)\n",
    "        end = time.time()\n",
    "        testingtime = end - start\n",
    "        print(\"Scoring time\", round(testingtime, 4), \"seconds\")\n",
    "    \n",
    "    \n",
    "        print(\"Dataset:\", ds_name)\n",
    "        print(\"Training Accuracy\", training_score)\n",
    "        print(\"Testing Accuracy\", testing_score)\n",
    "\n",
    "\n",
    "        # open a file, where you ant to store the decision classifier found after gridsearch\n",
    "        file = open(name_file, 'wb')\n",
    "\n",
    "        # dump information to that file\n",
    "        pickle.dump(clf, file)\n",
    "\n",
    "        # close the file\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    else:\n",
    "        #------------ taking the pickled decision tree out\n",
    "        # -------------- skip last cell -------------\n",
    "\n",
    "        file = open(name_file, 'rb')\n",
    "        clf = pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    return clf\n",
    "\n",
    "clf1 = prepruning_search(X_train1, y_train1, X_test1, y_test1, ds_name = 'census pay',\n",
    "                       need_training = True, name_file='dt_censuspay')\n",
    "print_tree_atts(clf1)\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "clf2 = prepruning_search(X_train2, y_train2, X_test2, y_test2, ds_name = 'body perf',\n",
    "                       need_training = True, name_file='dt_bodyperf')\n",
    "print_tree_atts(clf2)\n",
    "\n",
    "show_tree(X_train1, clf1, show=False, name=\"DTcensus_prepruned.png\")\n",
    "show_tree(X_train2, clf2, show=False, name=\"bodyperf_prepruned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77cad2",
   "metadata": {},
   "source": [
    "### Cost complexity pruning\n",
    "For this type of pruning, I will be making a separate validation set to set the value of alpha. \n",
    "Impurities increase as alphas increase because more of the trees are pruned and therefore in each leaf node, more of a mix of all outputs are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db053272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- code taken from https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html ----\n",
    "def need_val_test(X_test1, y_test1):\n",
    "    X_test11, X_val1, y_test11, y_val1 = train_test_split(X_test1, y_test1, test_size=0.4, random_state=38)\n",
    "    return X_test11, X_val1, y_test11, y_val1\n",
    "\n",
    "def impurities_alphas(clf, X_train1, y_train1, show=True):\n",
    "    res = clf.cost_complexity_pruning_path(X_train1, y_train1)\n",
    "    if show:\n",
    "        plt.plot(res['ccp_alphas'], res['impurities'], marker=\"o\", drawstyle=\"steps-post\", color='b')\n",
    "        plt.gca().set_xlabel('alphas')\n",
    "        plt.gca().set_ylabel('Impurities')\n",
    "        plt.title(\"How impurities grow bigger as alpha increases\")\n",
    "        plt.show()\n",
    "    return res\n",
    "\n",
    "\n",
    "def alphas_nodes_depth(res, X_train1, y_train1, show=True):\n",
    "    clfs = []\n",
    "    node_counts = []\n",
    "    depths = []\n",
    "    for ccp_alpha in res['ccp_alphas']:\n",
    "        clf = DecisionTreeClassifier(random_state=10, ccp_alpha=ccp_alpha)\n",
    "        clf.fit(X_train1, y_train1)\n",
    "        clfs.append(clf)\n",
    "        node_counts.append(clfs[-1].tree_.node_count)\n",
    "        depths.append(clfs[-1].tree_.max_depth)\n",
    "\n",
    "\n",
    "    # node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "    # depth = [clf.tree_.max_depth for clf in clfs]\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(2, 1)\n",
    "        ax[0].plot(res['ccp_alphas'], node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "        ax[0].set_xlabel(\"alpha\")\n",
    "        ax[0].set_ylabel(\"number of nodes\")\n",
    "        ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "        ax[0].set_ylim([0, 100])\n",
    "        ax[1].plot(res['ccp_alphas'], depths, marker=\"o\", drawstyle=\"steps-post\")\n",
    "        ax[1].set_xlabel(\"alpha\")\n",
    "        ax[1].set_ylabel(\"depth of tree\")\n",
    "        ax[1].set_title(\"Depth vs alpha\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    return clfs\n",
    "    \n",
    "def alpha_acc(clfs, X_train1, y_train1, X_val1, y_val1, res, limit):\n",
    "    train_scores = [clf.score(X_train1, y_train1) for clf in clfs]\n",
    "    test_scores = [clf.score(X_val1, y_val1) for clf in clfs]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"alpha\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "    ax.plot(res['ccp_alphas'], train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "    ax.plot(res['ccp_alphas'], test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "    if limit:\n",
    "        ax.set_xlim([0, limit])\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d069a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_all_cp(clf, X_train1, y_train1, X_test1, y_test1, limit):\n",
    "    X_test11, X_val1, y_test11, y_val1 = need_val_test(X_test1, y_test1)\n",
    "\n",
    "    res = impurities_alphas(clf, X_train1, y_train1, True)\n",
    "    plt.close(\"all\") #this is the line to be added\n",
    "\n",
    "    clfs = alphas_nodes_depth(res, X_train1, y_train1, False)\n",
    "    plt.close(\"all\") #this is the line to be added\n",
    "\n",
    "    alpha_acc(clfs, X_train1, y_train1, X_val1, y_val1, res, limit)\n",
    "    plt.close(\"all\") #this is the line to be added\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state = 50)\n",
    "run_all_cp(clf, X_train1, y_train1, X_test1, y_test1, 0.0005)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state = 50)\n",
    "run_all_cp(clf, X_train2, y_train2, X_test2, y_test2, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52862cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postpruning(X_train1, y_train1, X_test1, y_test1, perfect_a, d_name=\"census pay\", traintimes={}, \n",
    "                testtimes={}, accs={}):\n",
    "    clf = DecisionTreeClassifier(random_state=10, ccp_alpha = perfect_a)\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(X_train1, y_train1)\n",
    "    end = time.time()\n",
    "    trainingtime = end - start\n",
    "    print(\"Training time\", round(trainingtime, 4), \"seconds\")\n",
    "    traintimes['dt'] = trainingtime\n",
    "\n",
    "    training_score = clf.score(X_train1, y_train1)\n",
    "    \n",
    "    start = time.time()\n",
    "    testing_score = clf.score(X_test1, y_test1)\n",
    "    end = time.time()\n",
    "    testingtime = end - start\n",
    "    testtimes['dt'] = testingtime\n",
    "    print(\"Scoring time\", round(testingtime, 4), \"seconds\")\n",
    "    \n",
    "    print(\"Dataset\", d_name)\n",
    "    print(\"Training Accuracy\", training_score)\n",
    "    print(\"Testing Accuracy\", testing_score)\n",
    "    show_tree(X_train1, clf, show=False, name=\"DTcensus_postpruned.png\")\n",
    "    print_tree_atts(clf)\n",
    "    accs['dt'] = testing_score\n",
    "    return clf\n",
    "\n",
    "ppclf1 = postpruning(X_train1, y_train1, X_test1, y_test1, 0.00025,\"Census pay\", traintimesd1, testtimesd1, final_accsd1)\n",
    "print(\"----------------------------------------------------\")\n",
    "ppclf2 = postpruning(X_train2, y_train2, X_test2, y_test2, 0.0004, \"Body_performance\", traintimesd2, testtimesd2, final_accsd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83058714",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show_tree(X_train1, ppclf1, show=True, name=\"DTcensus_postpruned.png\")\n",
    "from sklearn import tree\n",
    "graph = Source( tree.export_graphviz(ppclf1, out_file=None, feature_names=X_train1.columns))\n",
    "SVG(graph.pipe(format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_tree(X_train1, ppclf1, show=True, name=\"DTcensus_postpruned.png\")\n",
    "\n",
    "clf1_ucp = DecisionTreeClassifier(random_state=10, max_depth=11)\n",
    "clf1_ucp.fit(X_train1, y_train1)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "graph = Source( tree.export_graphviz(clf1_ucp, out_file=None, feature_names=X_train1.columns))\n",
    "SVG(graph.pipe(format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac432e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##learning curves\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "clf_mss_2 = DecisionTreeClassifier(max_depth=6, random_state = 50, min_samples_split= 2)\n",
    "clf_mss_100  = DecisionTreeClassifier(max_depth=6,random_state = 50, min_samples_split= 100)\n",
    "plot_learning_curve_newdata(clf_mss_2, title, X_train1, y_train1, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Cenus pay\", estimator2=clf_mss_100, sp1='min_samples_split=2', \n",
    "                     sp2='min_samples_split=100', X2=X_train1, y2=y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##learning curves\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "plot_learning_curve_newdata(clf1_uc, title, X_train1, y_train1, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Cenus pay\", estimator2=ppclf1, sp1='Unconstrained', \n",
    "                     sp2='postpruned', X2=X_train1, y2=y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dee924",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "plot_learning_curve_newdata(clf2_uc, title, X_train2, y_train2, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Body Performance\", estimator2=ppclf2, sp1='Unconstrained', \n",
    "                     sp2='postpruned', X2=X_train2, y2=y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f23694",
   "metadata": {},
   "source": [
    "## KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eee76b",
   "metadata": {},
   "source": [
    "### Unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d51e483",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d81a6981b8ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_scaled1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_scaled1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train1, y_train1)\n",
    "x_scaled1 = scaler.transform(X_train1)\n",
    "x_scaled1 = pd.DataFrame(x_scaled1, columns=X_train1.columns)\n",
    "xtest_scaled1 = scaler.transform(X_test1)\n",
    "xtest_scaled1 = pd.DataFrame(xtest_scaled1, columns=X_train1.columns)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train2, y_train2)\n",
    "x_scaled2 = scaler.transform(X_train2)\n",
    "x_scaled2 = pd.DataFrame(x_scaled2, columns=X_train2.columns)\n",
    "xtest_scaled2 = scaler.transform(X_test2)\n",
    "xtest_scaled2 = pd.DataFrame(xtest_scaled2, columns=X_train2.columns)\n",
    "\n",
    "def scoring_wo_cons_knn(X_train1, y_train1, X_test1, y_test1):\n",
    "    neigh = KNeighborsClassifier()\n",
    "    start = time.time()\n",
    "    neigh.fit(X_train1, y_train1)\n",
    "    end = time.time()\n",
    "    trainingtime = end - start\n",
    "    print(\"Training time\", round(trainingtime, 4), \"seconds\")\n",
    "    \n",
    "    train_sc = neigh.score(X_train1, y_train1)\n",
    "    \n",
    "    start = time.time()\n",
    "    test_sc = neigh.score(X_test1, y_test1)\n",
    "    end = time.time()\n",
    "    testingtime = end - start\n",
    "    print(\"Testing time\", round(testingtime, 4), \"seconds\")\n",
    "    \n",
    "    print(\"Training score w/o GS\", train_sc)\n",
    "    print(\"Testing score w/o GS\", test_sc)\n",
    "    return neigh\n",
    "\n",
    "# ---------------------------------------------------------------- #\n",
    "\n",
    "print(\"Dataset1: Census pay\")\n",
    "clf1=scoring_wo_cons_knn(x_scaled1, y_train1, xtest_scaled1, y_test1)\n",
    "print(clf1)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Dataset2: Body performance\")\n",
    "clf2=scoring_wo_cons_knn(x_scaled2, y_train2, xtest_scaled2, y_test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4008bd2",
   "metadata": {},
   "source": [
    "#### Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295dedb",
   "metadata": {},
   "source": [
    "#### validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- dataset 1 ----------------------\n",
    "k_range = list(range(1,100,8))\n",
    "metric = ['hamming', 'minkowski', 'euclidean']\n",
    "# wei_range = ['uniform', 'distance']\n",
    "leaf_s_range = list(range(1,50,5))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "val_curve(knn, x_scaled1, y_train1, param_name='metric', param_range=metric, scoring = \"accuracy\", \n",
    "          ds_name='Census Pay')\n",
    "\n",
    "knn = KNeighborsClassifier(metric='minkowski')\n",
    "val_curve(knn, x_scaled1, y_train1, param_name='n_neighbors', param_range=k_range, scoring = \"accuracy\", \n",
    "          ds_name='Census Pay')\n",
    "\n",
    "\n",
    "# val_curve(knn, x_scaled1, y_train1, param_name='weights', param_range=wei_range, scoring = \"accuracy\", \n",
    "#           ds_name='Body Performance')\n",
    "\n",
    "\n",
    "knn1 = KNeighborsClassifier(metric = \"minkowski\", weights='uniform', n_neighbors=15)\n",
    "start = time.time()\n",
    "knn1.fit(x_scaled1, y_train1)\n",
    "end = time.time()\n",
    "trtime = end - start \n",
    "\n",
    "trs = knn1.score(x_scaled1, y_train1)\n",
    "s = time.time()\n",
    "ts = knn1.score(xtest_scaled1, y_test1)\n",
    "e = time.time()\n",
    "tt = e - s\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Prepruned knn for Dataset1: Census pay\")\n",
    "print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "print(\"Training score\", trs, \" Testing score\", ts)\n",
    "print(knn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb7226",
   "metadata": {},
   "source": [
    "### Testing theory that reducing features improves accuracy (Data 1)\n",
    "For dataset 1, selecting a few columns and testing with different values to check how different is the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "def knn_reduce_feats(X_train1, y_train1, X_test1, y_test1, method = \"columns\", columns = [], nfeats=8 ) :\n",
    "    if method == f_classif or method == chi2:\n",
    "        selector = SelectKBest(method, k=nfeats)\n",
    "        model = selector.fit(X_train1, y_train1)\n",
    "        cols = model.get_support(indices=True)\n",
    "        features_df_new = X_train1.iloc[:,cols]\n",
    "        print(features_df_new.columns)\n",
    "\n",
    "        X_new = model.transform(X_train1)\n",
    "        test_new = selector.transform(X_test1)\n",
    "    else:\n",
    "        X_new = X_train1[columns]\n",
    "        X_new.loc[:,'age'] = 0.7*X_new.loc[:,'age']\n",
    "        test_new = X_test1[columns]\n",
    "        test_new.loc[:,'age'] = 0.7* test_new.loc[:,'age']\n",
    "        \n",
    "        n = \"relationship\"\n",
    "        X_new.loc[:,n] =2*X_new.loc[:,n]\n",
    "        test_new.loc[:,n] = 2* test_new.loc[:,n]\n",
    "\n",
    "\n",
    "    neigh = KNeighborsClassifier()\n",
    "    params_to_tune = {'n_neighbors':[10, 15, 18, 25], 'weights': [ 'uniform'],\n",
    "                      'metric':['minkowski']\n",
    "                     }\n",
    "\n",
    "    rc = RandomizedSearchCV(neigh, params_to_tune,  n_iter=30, cv=5)\n",
    "    neigh = rc.fit(X_new, y_train1)\n",
    "\n",
    "    knn_best = neigh.best_estimator_\n",
    "    s=time.time()\n",
    "    knn_best.fit(X_new, y_train1)\n",
    "    e=time.time()\n",
    "    traintimesd1['knn']=(e-s)\n",
    "    tr_sc = knn_best.score(X_new, y_train1)\n",
    "    \n",
    "    s=time.time()\n",
    "    test_sc = knn_best.score(test_new, y_test1)\n",
    "    e=time.time()\n",
    "    \n",
    "    testtimesd1['knn']=(e-s)\n",
    "    final_accsd1['knn'] = test_sc\n",
    "    print(\"Training score:\", tr_sc, \" Testing score:\", test_sc)\n",
    "    \n",
    "    return knn_best, X_new\n",
    "\n",
    "print(\"My own 8 columns\")\n",
    "columns = ['age', 'sex', 'occupation', 'education-num', 'relationship', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "knn_best, X_new = knn_reduce_feats(X_train1, y_train1, X_test1, y_test1, \"columns\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b16d7",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f1e52",
   "metadata": {},
   "source": [
    "#### Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5a9df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -------------- dataset 2---------------------------\n",
    "k_range = list(range(1,100,8))\n",
    "metric = ['hamming', 'minkowski', 'euclidean']\n",
    "# wei_range = ['uniform', 'distance']\n",
    "leaf_s_range = list(range(1,50,5))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "val_curve(knn, x_scaled2, y_train2, param_name='metric', param_range=metric, scoring = \"accuracy\", \n",
    "          ds_name='Body Performance')\n",
    "\n",
    "knn = KNeighborsClassifier(metric='minkowski')\n",
    "val_curve(knn, x_scaled2, y_train2, param_name='n_neighbors', param_range=k_range, scoring = \"accuracy\", \n",
    "          ds_name='Body Performance')\n",
    "\n",
    "\n",
    "# val_curve(knn, x_scaled1, y_train1, param_name='weights', param_range=wei_range, scoring = \"accuracy\", \n",
    "#           ds_name='Body Performance')\n",
    "\n",
    "\n",
    "knn2 = KNeighborsClassifier(metric = \"minkowski\", weights='distance', n_neighbors=25)\n",
    "start = time.time()\n",
    "knn2.fit(x_scaled2, y_train2)\n",
    "end = time.time()\n",
    "trtime = end - start \n",
    "\n",
    "trs = knn2.score(x_scaled2, y_train2)\n",
    "s = time.time()\n",
    "ts = knn2.score(xtest_scaled2, y_test2)\n",
    "e = time.time()\n",
    "tt = e - s\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Prepruned knn for Dataset2: Body Performance\")\n",
    "print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "print(\"Training score\", trs, \" Testing score\", ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad554a",
   "metadata": {},
   "source": [
    "### Changing weights of the columns to achieve better results (Data 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age', 'gender', 'weight_kg', 'body fat_%', 'gripForce','sit and bend forward_cm', 'sit-ups counts', 'broad jump_cm' ]\n",
    "knn = KNeighborsClassifier(metric = \"minkowski\", weights='distance', n_neighbors=25)\n",
    "\n",
    "xscaled_sel = x_scaled2.loc[:, columns]\n",
    "xtest_scaled2_sel = xtest_scaled2.loc[:, columns]\n",
    "\n",
    "# 0.6294176207068193 if we give more importance to important feats\n",
    "c=2.8\n",
    "xscaled_sel['sit and bend forward_cm']= c*xscaled_sel['sit and bend forward_cm']\n",
    "xscaled_sel['sit-ups counts']=c*0.99*xscaled_sel['sit-ups counts']\n",
    "xscaled_sel['broad jump_cm']= 0.4*xscaled_sel['broad jump_cm']\n",
    "xscaled_sel['body fat_%']= 0.3*xscaled_sel['body fat_%']\n",
    "xscaled_sel['age']=2.3*xscaled_sel['age']\n",
    "\n",
    "\n",
    "\n",
    "xtest_scaled2_sel['sit and bend forward_cm']= c*xtest_scaled2_sel['sit and bend forward_cm']\n",
    "xtest_scaled2_sel['sit-ups counts']= c*0.99*xtest_scaled2_sel['sit-ups counts']\n",
    "xtest_scaled2_sel['broad jump_cm']= 0.4*xtest_scaled2_sel['broad jump_cm']\n",
    "xtest_scaled2_sel['body fat_%']= 0.3*xtest_scaled2_sel['body fat_%']\n",
    "xtest_scaled2_sel['age']= 2.3*xtest_scaled2_sel['age']\n",
    "\n",
    "knn_sc = KNeighborsClassifier(metric='minkowski', n_neighbors=15, weights='uniform')\n",
    "start = time.time()\n",
    "knn_sc.fit(xscaled_sel, y_train2)\n",
    "end = time.time()\n",
    "trtime = end - start \n",
    "traintimesd2['knn'] = trtime\n",
    "\n",
    "trs = knn_sc.score(xscaled_sel, y_train2)\n",
    "s = time.time()\n",
    "ts = knn_sc.score(xtest_scaled2_sel, y_test2)\n",
    "e = time.time()\n",
    "tt = e - s\n",
    "testtimesd2['knn'] = tt\n",
    "\n",
    "final_accsd2['knn'] = ts\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Prepruned knn for Dataset2: Body Performance\")\n",
    "print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "print(\"Training score\", trs, \" Testing score\", ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26bb39e",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f54e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "plot_learning_curve_newdata(estimator=knn1, title=title, X=x_scaled1, y=y_train1, axes=axes, ylim=None, cv=3,\n",
    "                     n_jobs=-1, scoring=\"accuracy\",train_sizes=np.linspace(0.1, 1.0, 5), d_name='Census pay',\n",
    "                     estimator2=knn_sc, sp1='No feature manipulation', sp2='Feature manipulation', X2=X_new,\n",
    "                     y2=y_train1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb800e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "knn2 = KNeighborsClassifier(metric = \"minkowski\", weights='distance', n_neighbors=25)\n",
    "knn_sc = KNeighborsClassifier(metric='minkowski', n_neighbors=15, weights='uniform')\n",
    "\n",
    "plot_learning_curve_newdata(estimator=knn2, title=title, X=x_scaled2, y=y_train2, axes=axes, ylim=None, cv=3,\n",
    "                     n_jobs=-1, scoring=\"accuracy\",train_sizes=np.linspace(0.1, 1.0, 5), d_name='Body Performance',\n",
    "                     estimator2=knn_sc, sp1='No feature manipulation', sp2='Feature manipulation', X2=xscaled_sel,\n",
    "                     y2=y_train2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e7be9",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745455a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#--- relu does better than tanh and logistic or use smaller learning rate for logistica nd tanh\n",
    "\n",
    "#----- scale the data --------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train1, y_train1)\n",
    "x_train1 = scaler.transform(X_train1)\n",
    "x_train1 = pd.DataFrame(x_train1, columns=X_train1.columns)\n",
    "x_test1 = scaler.transform(X_test1)\n",
    "x_test1 = pd.DataFrame(x_test1, columns=X_train1.columns)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train2, y_train2)\n",
    "x_train2 = scaler.transform(X_train2)\n",
    "x_train2 = pd.DataFrame(x_train2, columns=X_train2.columns)\n",
    "x_test2 = scaler.transform(X_test2)\n",
    "x_test2 = pd.DataFrame(x_test2, columns=X_train2.columns)\n",
    "\n",
    "#---- code taken from https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html ----\n",
    "def need_val_test(X_test1, y_test1):\n",
    "    X_test11, X_val1, y_test11, y_val1 = train_test_split(X_test1, y_test1, test_size=0.4, random_state=38)\n",
    "    return X_test11, X_val1, y_test11, y_val1\n",
    "x_test11, x_val1, y_test11, y_val1 = need_val_test(x_test1, y_test1)\n",
    "x_test22, x_val2, y_test22, y_val2 = need_val_test(x_test2, y_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc99e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['weight_kg', 'body fat_%', 'gripForce','sit and bend forward_cm', 'sit-ups counts', 'broad jump_cm' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = [\n",
    "    {'solver':'adam', 'learning_rate_init':0.001, 'learning_rate' : 'adaptive', 'activation':'relu'},\n",
    "    {'solver':'adam', 'learning_rate_init':0.0001, 'learning_rate': 'adaptive', 'activation' : 'logistic'},\n",
    "    {'solver':'adam', 'learning_rate_init':0.0001, 'learning_rate': 'adaptive', 'activation' : 'tanh'}\n",
    "]\n",
    "\n",
    "params = [\n",
    "    {'hidden_layer_sizes': (50, 50, 50, 50)},\n",
    "    {'hidden_layer_sizes': (25, 20, 20, 25)},\n",
    "    {'hidden_layer_sizes': (10, 10, 10, 10)},\n",
    "    \n",
    "    {'hidden_layer_sizes': (50, 50, 50)},\n",
    "    {'hidden_layer_sizes': (25, 20, 20)},\n",
    "    {'hidden_layer_sizes': (10, 10, 10)},\n",
    "    {'hidden_layer_sizes': (5, 5, 5)},\n",
    "    {'hidden_layer_sizes': (20, 10, 20)},\n",
    "    \n",
    "    {'hidden_layer_sizes': (100, 100)},\n",
    "    {'hidden_layer_sizes': (50, 50)},\n",
    "    {'hidden_layer_sizes': (25, 25)},\n",
    "    {'hidden_layer_sizes': (8, 8)},\n",
    "    {'hidden_layer_sizes': (10, 10)},\n",
    "    {'hidden_layer_sizes': (15, 15)},\n",
    "    \n",
    "    {'hidden_layer_sizes': (100)},\n",
    "    {'hidden_layer_sizes': (50)},\n",
    "    {'hidden_layer_sizes': (25)},\n",
    "]\n",
    "\n",
    "\n",
    "layers= []\n",
    "values_training = []\n",
    "values_testing = []\n",
    "for param in params:\n",
    "\n",
    "    clf = MLPClassifier(**param , random_state=10, max_iter=500, solver='adam', learning_rate_init=0.1, \n",
    "    learning_rate='adaptive',activation='relu')\n",
    "    clf.fit(x_train2, y_train2)\n",
    "    trs = clf.score(x_train2, y_train2)\n",
    "    ts = clf.score(x_val2, y_val2)\n",
    "    print(\"Training score:\", trs, \" validation score:\", ts, param['hidden_layer_sizes'])\n",
    "    layers.append(str(param['hidden_layer_sizes']))\n",
    "    values_training.append(trs)\n",
    "    values_testing.append(ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------  plotting ---------     \n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "X = np.arange(0, len(layers))\n",
    "\n",
    "ax.bar(layers, values_training, color = 'b', width = 0.25, label = 'Training Accuracy')\n",
    "ax.bar(X+0.25, values_testing, color = 'g', width = 0.25, label = \"Validation Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_range = [0.001, 0.01, 0.1, 10]\n",
    "clf_1 = MLPClassifier(random_state=10, max_iter=300, solver='adam', learning_rate_init=0.001,activation='relu', \n",
    "                      hidden_layer_sizes =(10, 15))\n",
    "\n",
    "\n",
    "val_curve(clf_1, x_train1, y_train1, \"learning_rate_init\", l_range, \"accuracy\", \"Census pay\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf_1 = MLPClassifier(random_state=10, max_iter=500, solver='adam', learning_rate_init=0.001,activation='relu', \n",
    "                      hidden_layer_sizes =(10, 15))\n",
    "\n",
    "clf_2 = MLPClassifier(random_state=10, max_iter=500, solver='adam', learning_rate_init=0.001,activation='relu', hidden_layer_sizes =(10, 10))\n",
    "\n",
    "def start_training (clf, x_train1, y_train1, x_test11, y_test11, dname='Census pay', trti={}, tsti={}, acc={}, extra_clf = False ):\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(x_train1, y_train1)\n",
    "    end = time.time()\n",
    "    trtime = end - start\n",
    "    trti['nn'] = trtime\n",
    "    \n",
    "    if extra_clf == True:\n",
    "        clf2=MLPClassifier(random_state=10, max_iter=500, solver='adam', learning_rate_init=0.01, \n",
    "        learning_rate='adaptive',activation='relu', hidden_layer_sizes =(10, 15))\n",
    "        clf2.fit(x_train1, y_train1)\n",
    "        trs2 = clf2.score(x_train1, y_train1)\n",
    "        ts2 = clf2.score(x_test11, y_test11)\n",
    "        l2 = 'Learning_rate = '+ str(clf2.learning_rate_init)\n",
    "        print(\"score with clf2\", trs2, ts2)\n",
    "\n",
    "    trs = clf.score(x_train1, y_train1)\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    ts = clf.score(x_test11, y_test11)\n",
    "    end = time.time()\n",
    "    tstime = end - start\n",
    "    tsti['nn'] = tstime\n",
    "    acc['nn'] = ts\n",
    "    \n",
    "\n",
    "    print(\"Training score:\", trs, \" validation score:\", ts)\n",
    "    print(\"Training time\", round(trtime, 4), \" Testing time\", round(tstime, 4), \"seconds\")\n",
    "\n",
    "    l1 = 'Learning_rate = '+ str(clf.learning_rate_init)\n",
    "\n",
    "    plt.plot(clf.loss_curve_, label=l1)\n",
    "    if extra_clf == True:\n",
    "        plt.plot(clf2.loss_curve_, label=l2)\n",
    "    tit = \"Loss curves for Dataset: \" + dname \n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title(tit)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "start_training(clf_1, x_train1, y_train1, x_test11, y_test11, 'Census pay', traintimesd1, testtimesd1, final_accsd1, True) \n",
    "start_training(clf_2, x_train2, y_train2, x_test22, y_test22, \"Body Performance\", traintimesd2,testtimesd2, final_accsd2, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f30a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- analyzing iterations and loss --------------\n",
    "\n",
    "N_TRAIN_SAMPLES = x_train2.shape[0]\n",
    "N_EPOCHS = 900\n",
    "N_BATCH = x_train2.shape[0]\n",
    "N_CLASSES = np.unique(y_train2)\n",
    "\n",
    "scores_train1 = []\n",
    "scores_test1 = []\n",
    "\n",
    "scores_train2 = []\n",
    "scores_test2 = []\n",
    "       \n",
    "    \n",
    "mlp1 = MLPClassifier(random_state=10, max_iter=1, solver='adam', learning_rate_init=0.001,\n",
    "                    activation='relu', hidden_layer_sizes =(10, 10), warm_start=True)\n",
    "mlp2 = MLPClassifier(random_state=10, max_iter=1, solver='adam', learning_rate_init=0.001,\n",
    "                    activation='relu', hidden_layer_sizes =(30, 30), warm_start=True)\n",
    "# EPOCH\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    mlp1.partial_fit(x_train2, y_train2, N_CLASSES)\n",
    "    scores_train1.append(1-mlp1.score(x_train2, y_train2))\n",
    "    scores_test1.append(1-mlp1.score(x_val2, y_val2))\n",
    "    \n",
    "    mlp2.partial_fit(x_train2, y_train2, N_CLASSES)\n",
    "    scores_train2.append(1-mlp2.score(x_train2, y_train2))\n",
    "    scores_test2.append(1-mlp2.score(x_val2, y_val2))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "plt.plot(scores_train1, color='green', alpha=0.8, label='Train-(10,10)', linestyle= '--')\n",
    "plt.plot(scores_test1, color='magenta', alpha=0.8, label='Test-(10,10)', linestyle ='--')\n",
    "plt.plot(scores_train2, color='blue', alpha=1, label='Train-(30,30)', linestyle=':')\n",
    "plt.plot(scores_test2, color='black', alpha=1, label='Test-(30,30)',linestyle=':')\n",
    "\n",
    "plt.title(\"Error over epochs; \", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('% Error')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4be72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=MLPClassifier(random_state=10, max_iter=500, solver='adam', learning_rate_init=0.01, \n",
    "    activation='relu', hidden_layer_sizes =(15, 15, 15, 15))\n",
    "clf2=MLPClassifier(random_state=10, max_iter=500, solver='adam', learning_rate_init=0.001, \n",
    "    activation='relu', hidden_layer_sizes =(50, 50))\n",
    "clf_2 = MLPClassifier(random_state=10, max_iter=500, solver='adam', learning_rate_init=0.1, \n",
    "    activation='relu', hidden_layer_sizes =(10, 10))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "plot_learning_curve_newdata(clf_1, title, X= x_train1, y=y_train1, axes=axes, ylim=None, cv=3,  n_jobs=-1,\n",
    "                            scoring=\"accuracy\",  train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Census pay\",\n",
    "                            estimator2=clf1, sp1='2 layers', sp2='4 layers', X2=x_train1,\n",
    "                            y2=y_train1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "plot_learning_curve_newdata(clf_2, title, X= x_train2, y=y_train2, axes=axes, ylim=None, cv=3,  n_jobs=-1,\n",
    "                            scoring=\"accuracy\",  train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Body Performance\",\n",
    "                            estimator2=clf2, sp1='Hidden layers (10, 10)', sp2='Hidden layers (90, 90)', X2=x_train2,\n",
    "                            y2=y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972ebb0",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "#### Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svl = SVC(kernel='linear')\n",
    "c_range =[0.01, 0.1, 1, 2,  10]\n",
    "\n",
    "val_curve(svl, x_train2, y_train2, \"C\", c_range, \"accuracy\", \"Body Performance\")\n",
    "val_curve(svl, x_train1, y_train1, \"C\", c_range, \"accuracy\", \"Census pay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train1, x_test1, x_train2, x_test2 \n",
    "svl = SVC(kernel='linear', C=2)\n",
    "\n",
    "\n",
    "def support_vector(clf, x_scaled1, y_train1, xtest_scaled1, y_test1, dname='Census pay'):\n",
    "    start = time.time()\n",
    "    clf.fit(x_scaled1, y_train1)\n",
    "    end = time.time()\n",
    "    trtime = end - start \n",
    "    \n",
    "    trs = clf.score(x_scaled1, y_train1)\n",
    "    s = time.time()\n",
    "    ts = clf.score(xtest_scaled1, y_test1)\n",
    "    e = time.time()\n",
    "    tt = e - s\n",
    "\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"SVM for Dataset\", dname)\n",
    "    print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "    print(\"Training score\", trs, \" Testing score\", ts)\n",
    "    \n",
    "    \n",
    "support_vector(svl, x_scaled1, y_train1, xtest_scaled1, y_test1)\n",
    "support_vector(svl, x_scaled2, y_train2, xtest_scaled2, y_test2, \"Body Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b41e4",
   "metadata": {},
   "source": [
    "#### RBF kernel of the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31baf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_train1, x_test1, x_train2, x_test2 \n",
    "\n",
    "svr = SVC(kernel='rbf')\n",
    "c_range = np.linspace(0.01, 10, 5)\n",
    "g_range = [0.001, 0.006, 0.05, 0.1, 0.2]\n",
    "\n",
    "val_curve(svr, x_train1, y_train1, \"C\", c_range, \"accuracy\", \"Census pay\")\n",
    "val_curve(svr, x_train2, y_train2, \"C\", c_range, \"accuracy\", \"Body Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe91158",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVC(kernel='rbf', C=3)\n",
    "\n",
    "val_curve(svr, x_train1, y_train1, \"gamma\",g_range, \"accuracy\", \"Census pay\", False)\n",
    "val_curve(svr, x_train2, y_train2, \"gamma\",g_range, \"accuracy\", \"Body Perfornance\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svr1 = SVC(kernel='rbf', C=3, gamma=0.01)\n",
    "svr2 = SVC(kernel='rbf', C=3, gamma=0.1)\n",
    "\n",
    "def support_vector(clf, x_scaled1, y_train1, xtest_scaled1, y_test1, dname='Census pay', trt={}, tst={}, acc = {}):\n",
    "    start = time.time()\n",
    "    clf.fit(x_scaled1, y_train1)\n",
    "    end = time.time()\n",
    "    trtime = end - start \n",
    "    trt['svm']=trtime\n",
    "    \n",
    "    trs = clf.score(x_scaled1, y_train1)\n",
    "    s = time.time()\n",
    "    ts = clf.score(xtest_scaled1, y_test1)\n",
    "    e = time.time()\n",
    "    tt = e - s\n",
    "    tst['svm'] = tt\n",
    "    \n",
    "    acc['svm'] = ts\n",
    "\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"SVM for Dataset\", dname)\n",
    "    print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "    print(\"Training score\", trs, \" Testing score\", ts)\n",
    "\n",
    "support_vector(svr1, x_scaled1, y_train1, xtest_scaled1, y_test1, \"Census pay\", traintimesd1, testtimesd1, final_accsd1)\n",
    "print(\"-----------------------------------------------------------\")\n",
    "support_vector(svr2, x_scaled2, y_train2, xtest_scaled2, y_test2, \"Body Performance\", traintimesd2, testtimesd2, final_accsd2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "plot_learning_curve_newdata(svl, title, X= x_scaled1, y=y_train1, axes=axes, ylim=None, cv=3,  n_jobs=-1,\n",
    "                            scoring=\"accuracy\",  train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Census data\",\n",
    "                            estimator2=svr, sp1='linear', sp2='rbf', X2=x_scaled1,\n",
    "                            y2=y_train1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46280af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "svr2 = SVC(kernel='rbf', C=3, gamma=0.1)\n",
    "\n",
    "plot_learning_curve_newdata(svl, title, X= x_scaled2, y=y_train2, axes=axes, ylim=None, cv=3,  n_jobs=-1,\n",
    "                            scoring=\"accuracy\",  train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Body Performance\",\n",
    "                            estimator2=svr2, sp1='linear', sp2='rbf', X2=x_scaled2,  y2=y_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "svl = SVC(kernel='linear', C=2)\n",
    "svl2 = SVC(kernel='linear', C=0.01)\n",
    "\n",
    "plot_learning_curve_newdata(svl, title, x_scaled2, y_train2, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Body Performance\", estimator2=svl2, sp1='linear: C=2', \n",
    "                     sp2='linear: C=0.01', X2=x_scaled2, y2=y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38136dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "svr01 = SVC(kernel='rbf', C=3, gamma=0.1)\n",
    "svr1 = SVC(kernel='rbf', C=3, gamma=1)\n",
    "\n",
    "plot_learning_curve_newdata(svr01, title, x_scaled2, y_train2, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Body Performance\", estimator2=svr1, sp1='rbf gamma=0.1', \n",
    "                     sp2='rbf gamma=1', X2=x_scaled2, y2=y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f98c5d",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee534b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100,  learning_rate=1,  \n",
    "#                         algorithm=\"SAMME\",)\n",
    "ab = AdaBoostClassifier()\n",
    "\n",
    "def adaboost(clf, x_scaled1, y_train1, xtest_scaled1, y_test1, dname='Census pay'):\n",
    "    start = time.time()\n",
    "    clf.fit(x_scaled1, y_train1)\n",
    "    end = time.time()\n",
    "    trtime = end - start \n",
    "\n",
    "    trs = clf.score(x_scaled1, y_train1)\n",
    "    s = time.time()\n",
    "    ts = clf.score(xtest_scaled1, y_test1)\n",
    "    e = time.time()\n",
    "    tt = e - s\n",
    "\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"Adaboost for Dataset\", dname)\n",
    "    print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "    print(\"Training score\", trs, \" Testing score\", ts)\n",
    "\n",
    "\n",
    "adaboost(ab, X_train1, y_train1, X_test1, y_test1)\n",
    "print(\"-----------------------------------------------------------\")\n",
    "adaboost(ab, X_train2, y_train2, X_test2, y_test2, \"Body Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22862773",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier()\n",
    "max_depth_range = list(range(1, 10, 1))\n",
    "dcs = [DecisionTreeClassifier(max_depth=k) for k in max_depth_range]\n",
    "e_range=list(range(0, 300, 10))\n",
    "val_curve2(ab, x_train1, y_train1, \"base_estimator\", dcs, \"accuracy\", \"Census pay\", max_depth_range)\n",
    "\n",
    "ab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2))\n",
    "val_curve(ab, x_train1, y_train1, \"n_estimators\", e_range, \"accuracy\", \"Census pay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05131d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_curve2(ab, X_train2, y_train2, \"base_estimator\", dcs, \"accuracy\", \"Body performance\", max_depth_range)\n",
    "ab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3))\n",
    "e_range=list(range(0, 55, 2))\n",
    "val_curve(ab, X_train2, y_train2, \"n_estimators\", e_range, \"accuracy\", \"Body performance data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fccfcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators = 50)\n",
    "ab2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators = 12)\n",
    "\n",
    "def adaboost(clf, x_scaled1, y_train1, xtest_scaled1, y_test1, dname='Census pay', trt={}, tst ={}, acc={}):\n",
    "    start = time.time()\n",
    "    clf.fit(x_scaled1, y_train1)\n",
    "    end = time.time()\n",
    "    trtime = end - start \n",
    "    trt['ab'] = trtime\n",
    "\n",
    "    trs = clf.score(x_scaled1, y_train1)\n",
    "    s = time.time()\n",
    "    ts = clf.score(xtest_scaled1, y_test1)\n",
    "    e = time.time()\n",
    "    tt = e - s\n",
    "    tst['ab'] = tt\n",
    "    acc['ab'] = ts\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"Adaboost for Dataset\", dname)\n",
    "    print(\"Training time\", round(trtime, 4), \" Testing time\", round(tt, 4), \"seconds\")\n",
    "    print(\"Training score\", trs, \" Testing score\", ts)\n",
    "\n",
    "adaboost(ab1, X_train1, y_train1, X_test1, y_test1, \"Census pay\", traintimesd1, testtimesd1, final_accsd1)\n",
    "print(\"-----------------------------------------------------------\")\n",
    "adaboost(ab2, X_train2, y_train2, X_test2, y_test2, \"Body Performance\", traintimesd2, testtimesd2, final_accsd2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "ab2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators = 12)\n",
    "\n",
    "plot_learning_curve_newdata(ab, title, x_scaled2, y_train2, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Body Performance\", estimator2=ab2, sp1='Unconstrained', \n",
    "                     sp2='Tuned', X2=x_scaled2, y2=y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "title=\"Learning Curves\"\n",
    "\n",
    "plot_learning_curve_newdata(ab, title, x_scaled1, y_train1, axes=axes, ylim=None, cv=3,  n_jobs=-1, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), d_name=\"Census data\", estimator2=ab1, sp1='Unconstrained', \n",
    "                     sp2='Tuned', X2=x_scaled1, y2=y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c335158",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "labels = ['Decision Tree', 'Knn', 'Neural Networks', 'SVM (rbf)', 'AdaBoost']\n",
    "\n",
    "font_color = '#525252'\n",
    "hfont = {'fontname':'Calibri'}\n",
    "facecolor = '#eaeaf2'\n",
    "color_red = '#fd625e'\n",
    "color_red2 = '#fd0000'\n",
    "\n",
    "title0 = 'Training Times (s)'\n",
    "title1 = 'Testing Times (s)'\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15,5), facecolor=facecolor, ncols=2, sharey=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "axes[0].barh(X, traintimesd1.values(), align='center', color=color_red2, zorder=10, height=0.4, label = 'DS1')\n",
    "axes[0].barh(X+0.4, traintimesd2.values(), align='center', color=color_red, zorder=10,  height=0.4, label='DS2')\n",
    "axes[0].set_title(title0, fontsize=10, pad=5, color='b', **hfont)\n",
    "\n",
    "\n",
    "axes[1].barh(X, testtimesd1.values(), align='center', color='darkslategray', zorder=10, height=0.4, label='DS1')\n",
    "axes[1].barh(X+0.4, testtimesd2.values(), align='center', color='slategray', zorder=10, height=0.4, label='DS2')\n",
    "axes[1].set_title(title11, fontsize=10, pad=15, color='b', **hfont)\n",
    "\n",
    "\n",
    "plt.yticks(X, labels)\n",
    "plt.subplots_adjust(wspace=0, top=0.85, bottom=0.1, left=0.18, right=0.95)\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "axes[1].set_xlim(0, 10)\n",
    "axes[0].invert_xaxis() \n",
    "\n",
    "# axes[0].set(yticks=data.index, yticklabels=data.index)\n",
    "# axes[0].yaxis.tick_left()\n",
    "# axes[0].tick_params(axis='x', colors='white') # tick color\n",
    "# axes[1].tick_params(axis='x', colors='white') # tick color\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax = plt.gca()\n",
    "X = np.arange(0, len(final_accsd1))\n",
    "\n",
    "ax.bar(X, final_accsd1.values(), color = 'b', width = 0.2, label = 'DS 1')\n",
    "ax.bar(X+0.20,final_accsd2.values(), color = 'g', width = 0.2, label = \"DS 2\")\n",
    "plt.xticks(X, labels)\n",
    "plt.title('Best accuracies and Models')\n",
    "for i, v in enumerate(final_accsd1.values()):\n",
    "    plt.text(X[i] - 0.15, v + 0.01, str(round(v,3)))\n",
    "for i, v in enumerate(final_accsd2.values()):\n",
    "    plt.text(X[i]+0.2 , v + 0.01, str(round(v,3)))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d08ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
